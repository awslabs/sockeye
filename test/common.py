# Copyright 2017--2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You may not
# use this file except in compliance with the License. A copy of the License
# is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is distributed on
# an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
import os
import sys
from typing import Any, Dict, List
from unittest.mock import patch

import numpy as np

import sockeye.score
import sockeye.translate
from sockeye import constants as C
from sockeye.test_utils import run_train_translate, run_translate_restrict, TRANSLATE_PARAMS_COMMON, \
    TRANSLATE_WITH_FACTORS_COMMON, collect_translate_output_and_scores, create_reference_constraints, \
    SCORE_PARAMS_COMMON, SCORE_WITH_FACTORS_COMMON

logger = logging.getLogger(__name__)


def check_train_translate(train_params: str,
                          translate_params: str,
                          data: Dict[str, Any],
                          use_prepared_data: bool,
                          max_seq_len: int,
                          compare_output: bool = True,
                          seed: int = 13) -> Dict[str, Any]:
    """
    Tests core features (training, inference).
    """
    # train model and translate test set
    data = run_train_translate(train_params=train_params,
                               translate_params=translate_params,
                               data=data,
                               use_prepared_data=use_prepared_data,
                               max_seq_len=max_seq_len,
                               seed=seed)

    # Test equivalence of batch decoding
    translate_params_batch = translate_params + " --batch-size 2"
    test_translate_equivalence(data, translate_params_batch, compare_output)

    # Run translate with restrict-lexicon
    data = run_translate_restrict(data, translate_params)

    # Test scoring by ensuring that the sockeye.scoring module produces the same scores when scoring the output
    # of sockeye.translate. However, since this training is on very small datasets, the output of sockeye.translate
    # is often pure garbage or empty and cannot be scored. So we only try to score if we have some valid output
    # to work with.
    # Only run scoring under these conditions. Why?
    # - translate splits up too-long sentences and translates them in sequence, invalidating the score, so skip that
    # - scoring requires valid translation output to compare against
    if '--max-input-length' not in translate_params and _translate_output_is_valid(data['test_outputs']):
        test_scoring(data, translate_params, compare_output)

    return data


def test_translate_equivalence(data: Dict[str, Any], translate_params_equiv: str, compare_output: bool):
    """
    Tests whether the output and scores generated by sockeye.translate with translate_params_equiv are equal to
    the previously generated outputs, referenced in the data dictionary.
    """
    out_path = os.path.join(data['work_dir'], "test.out.equiv")
    params = "{} {} {}".format(sockeye.translate.__file__,
                               TRANSLATE_PARAMS_COMMON.format(model=data['model'],
                                                              input=data['test_source'],
                                                              output=out_path),
                               translate_params_equiv)
    if 'test_source_factors' in data:
        params += TRANSLATE_WITH_FACTORS_COMMON.format(input_factors=" ".join(data['test_source_factors']))
    with patch.object(sys, "argv", params.split()):
        sockeye.translate.main()
    # Collect translate outputs and scores
    translate_outputs_equiv, translate_scores_equiv = collect_translate_output_and_scores(out_path)

    assert 'test_outputs' in data and 'test_scores' in data
    assert len(data['test_outputs']) == len(translate_outputs_equiv)
    assert len(data['test_scores']) == len(translate_scores_equiv)
    if compare_output:
        assert all(a == b for a, b in zip(data['test_outputs'], translate_outputs_equiv))
        assert all(abs(a - b) < 0.01 or np.isnan(a - b) for a, b in zip(data['test_scores'], translate_scores_equiv))


def test_constrained_decoding_against_ref(data: Dict[str, Any], translate_params: str):
    constrained_inputs = create_reference_constraints(data['test_inputs'], data['test_outputs'])
    new_test_source_path = os.path.join(data['work_dir'], "test_constrained.txt")
    with open(new_test_source_path, 'w') as out:
        for json_line in constrained_inputs:
            print(json_line, file=out)
    out_path_constrained = os.path.join(data['work_dir'], "out_constrained.txt")
    params = "{} {} {} --json-input --output-type translation_with_score --beam-size 1 --batch-size 1 --nbest-size 1".format(
        sockeye.translate.__file__,
        TRANSLATE_PARAMS_COMMON.format(model=data['model'],
                                       input=new_test_source_path,
                                       output=out_path_constrained),
        translate_params)
    with patch.object(sys, "argv", params.split()):
        sockeye.translate.main()
    constrained_outputs, constrained_scores = collect_translate_output_and_scores(out_path_constrained)
    assert len(constrained_outputs) == len(data['test_outputs']) == len(constrained_inputs)
    for json_input, constrained_out, unconstrained_out in zip(constrained_inputs, constrained_outputs, data['test_outputs']):
        # Make sure the constrained output is the same as we got when decoding unconstrained
        assert constrained_out == unconstrained_out

    data['test_constrained_inputs'] = constrained_inputs
    data['test_constrained_outputs'] = constrained_outputs
    data['test_constrained_scores'] = constrained_scores
    return data


def test_scoring(data: Dict[str, Any], translate_params: str, test_similar_scores: bool):
    """
    Tests the scoring CLI and checks for score equivalence with previously generated translate scores.
    """
    # Translate params that affect the score need to be used for scoring as well.
    relevant_params = {'--brevity-penalty-type',
                       '--brevity-penalty-weight',
                       '--brevity-penalty-constant-length-ratio',
                       '--length-penalty-alpha',
                       '--length-penalty-beta'}
    score_params = ''
    params = translate_params.split()
    for i, param in enumerate(params):
        if param in relevant_params:
            score_params = '{} {}'.format(param, params[i + 1])
    out_path = os.path.join(data['work_dir'], "score.out")

    # write translate outputs as target file for scoring and collect tokens
    target_path = os.path.join(data['work_dir'], "score.target")
    translate_tokens = []
    with open(target_path, 'w') as target_out:
        for output in data['test_outputs']:
            print(output, file=target_out)
            translate_tokens.append(output.split())

    params = "{} {} {}".format(sockeye.score.__file__,
                               SCORE_PARAMS_COMMON.format(model=data['model'],
                                                          source=data['test_source'],
                                                          target=target_path,
                                                          output=out_path),
                               score_params)
    if 'test_source_factors' in data:
        params += SCORE_WITH_FACTORS_COMMON.format(source_factors=" ".join(data['test_source_factors']))
    logger.info("Scoring with params %s", params)
    with patch.object(sys, "argv", params.split()):
        sockeye.score.main()

    # Collect scores from output file
    with open(out_path) as score_out:
        score_scores = [float(line.strip()) for line in score_out]

    if test_similar_scores:
        for inp, translate_tokens, translate_score, score_score in zip(data['test_inputs'],
                                                                       translate_tokens,
                                                                       data['test_scores'],
                                                                       score_scores):
            logger.info("tokens: %s || translate score: %.4f || score score: %.4f",
                        translate_tokens, translate_score, score_score)
            assert (translate_score == -np.inf and score_score == -np.inf) or np.isclose(translate_score,
                                                                                         score_score,
                                                                                         atol=1e-06),\
                "input: %s || tokens: %s || translate score: %.6f || score score: %.6f" % (inp, translate_tokens,
                                                                                           translate_score,
                                                                                           score_score)


def _translate_output_is_valid(translate_outputs: List[str]) -> bool:
    """
    True if there are invalid tokens in out_path, or if no valid outputs were found.
    """
    # At least one output must be non-empty
    found_valid_output = False
    bad_tokens = set(C.VOCAB_SYMBOLS)
    for output in translate_outputs:
        if output:
            found_valid_output = True
        if any(token for token in output.split() if token in bad_tokens):
            # There must be no bad tokens
            return False
    return found_valid_output
